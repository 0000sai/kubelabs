# Managed Kubernetes Service

Setting up a simple cluster on an existing Linux server is not particularly complicated. You need to get a master node that acts as the control plane, and worker nodes where your actual cluster will run. Then you can use the official documentation to set up a basic cluster using the Kube API server, Kube proxy, and a Docker runtime. If you were in a development environment, you could go ahead and use something like Minikube to set up a one-node cluster with a single command. However, none of this will work with enterprise-level clusters due to several reasons. First of all, one of the biggest disadvantages of microservices is that there is a higher probability of a security breach, and companies go to the extent of hiring a full security team to handle that part alone. If you're interested in seeing the amount of work that goes into securing clusters, head over to the [Security101 section](../Security101/devsecops.md). However, even if you were a freelance developer creating a cluster for a small client, an on-prem cluster will still restrict you. If you have a database, you would need to configure data persistence and fault tolerance. You need to back up your own clusters, scale your clusters by adding servers, and do all the management of the cluster yourself.

On the other hand, a managed Kubernetes service does all this for you. First of all, let's consider the master node.

In most managed Kubernetes services, the master node is completely handled by the service. This means that you won't even see or note that the master node exists. It gets created when you create your worker nodes and run in the background with no interference from you. Also, in the case of services such as Microsoft's AKS or Linode's LKE, the master node is free. This means you no longer have to maintain the resources to run or manage this master node.

Secondly, when it comes to the worker nodes, you don't have to bother with setting up the docker runtime, Kube-api server, Kube proxy, etc... You only have to specify how many worker nodes you want via either the CLI or the cloud portal of the relevant service.